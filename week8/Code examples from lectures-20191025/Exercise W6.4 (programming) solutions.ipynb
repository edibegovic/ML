{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise W6.4 (programming)\n",
    "\n",
    "We consider the wine data set again. For this exercise we have provided a training set and a test set with filenames:\n",
    "\n",
    "wine X train.txt, wine t train.txt and wine X test.txt, wine t test.txt.\n",
    "\n",
    "We consider all three classes, where Barolo is class $1$ or $(1, 0, 0)$, Grignolino is class $2$ or $(0, 1, 0)$, and Barbera is class $3$ or $(0, 0, 1)$. Start by loading in the dataset (using np.loadtxt).\n",
    "\n",
    "        \n",
    "First, we will implement multiclass logistic regression from section 4.3.4 in Bishop using TensorFlow. We will use the identity basis function\n",
    "\n",
    "$\\phi(x) = x$\n",
    "\n",
    "and explicitly add a bias term. This means that we can write the activations from equation (4.105) as \n",
    "\n",
    "$a_k = w_{k}^T 􏰈x + b_k$.\n",
    "\n",
    "It is useful to implement this equation as\n",
    "    $a=xW +b$ (W4.5) where $a = (a_1,...,a_K)$􏰈, $W = (w_1,...,w_K) and $b = (b_1,...,b_K)^T$􏰈.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a)** \n",
    "Start by implementing the TensorFlow graph for the model, i.e. placeholders to the input x and the target t, the weights W and biases b, the activation and class probabilities. Initialize the weights with from a Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"wine/wine_X_train.txt\", names=[\"alc\",\"acid\"],delimiter=\" \")\n",
    "X_test = pd.read_csv(\"wine/wine_X_test.txt\", names=[\"alc\",\"acid\"], delimiter=\" \")\n",
    "t_train = pd.read_csv(\"wine/wine_t_train.txt\",names=[\"Barolo\",\"Gringolino\",\"Barbera\"] ,delimiter=\" \")\n",
    "t_test = pd.read_csv(\"wine/wine_t_test.txt\",names=[\"Barolo\",\"Gringolino\",\"Barbera\"], delimiter=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input, putput, weights and bias\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 3])\n",
    "xt = tf.placeholder(tf.float32,[None, 2])\n",
    "tt = tf.placeholder(tf.float32,[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined the model parameters\n",
    "W = tf.get_variable(\"W\", [X_train.shape[1],t_train.shape[1]], initializer=tf.random_normal_initializer)\n",
    "b = tf.get_variable(\"b\", [t_train.shape[1]], initializer=tf.random_normal_initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b)**\n",
    "Implement the loss function using tf.nn.softmax cross entropy with logits v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "y = tf.matmul(x,W) + b\n",
    "yt = tf.nn.softmax(tf.matmul(xt,W) + b)\n",
    "\n",
    "# Difine the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(y, 1)\n",
    "prediction_test = tf.argmax(yt, 1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(t, 1)), tf.float32))\n",
    "accuracy_test = tf.reduce_mean(tf.cast(tf.equal(prediction_test, tf.argmax(tt, 1)), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c)** \n",
    "Training the model using batch gradient decent (GradientDescentOptimizer) and\n",
    "learning rate of 0.0001 and for 50000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/viktortorpthomsen/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  loss = 123.320671\n",
      "Epoch: 10000  loss = 1.012405  diff = 0.000032306\n",
      "Epoch: 20000  loss = 0.899174  diff = 0.000003338\n",
      "Epoch: 30000  loss = 0.883531  diff = 0.000001192\n",
      "Epoch: 40000  loss = 0.879841  diff = 0.000000060\n",
      "Epoch: 49999  loss = 0.878471  diff = 0.000000000\n",
      "Optimization done\n",
      "Accuracy on train set: 0.5422535\n",
      "Accuracy on test set: 0.3611111\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Start a new session\n",
    "with tf.Session() as session:\n",
    "    # Initialize the values\n",
    "    session.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(50000):\n",
    "        _, loss_value = session.run([optimizer, loss], feed_dict={x: X_train, t: t_train.as_matrix()})\n",
    "        \n",
    "        if epoch % 10000 == 0 or epoch == 49999:\n",
    "            if epoch > 1:\n",
    "                print(\"Epoch: {}  loss = {:.6f}  diff = {:.9f}\".format(epoch,loss_value,prev-loss_value))\n",
    "            else:\n",
    "                print(\"Epoch: {}  loss = {:.6f}\".format(epoch,loss_value))\n",
    "        prev = loss_value            \n",
    "            \n",
    "    print(\"Optimization done\")\n",
    "\n",
    "    accuracy_value = session.run(accuracy, feed_dict={x: X_train, t: t_train})\n",
    "    print(\"Accuracy on train set:\", accuracy_value)\n",
    "    accuracy_value = session.run(accuracy_test, feed_dict={xt: X_test, tt: t_test})\n",
    "    print(\"Accuracy on test set:\", accuracy_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement a two layer neural network with 5 hidden nodes and the rectifier activation function for the hidden layer following the same steps as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two layed neural network\n",
    "# Input and output\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "t = tf.placeholder(tf.float32, [None, 3])\n",
    "xt = tf.placeholder(tf.float32,[None, 2])\n",
    "tt = tf.placeholder(tf.float32,[None, 3])\n",
    "\n",
    "# Defined the model parameters\n",
    "W1 = tf.get_variable(\"W1\", [2, 5], initializer=tf.random_normal_initializer)\n",
    "b1 = tf.get_variable(\"b1\", [5], initializer=tf.random_normal_initializer)\n",
    "W2 = tf.get_variable(\"W2\", [5, 3], initializer=tf.random_normal_initializer)\n",
    "b2 = tf.get_variable(\"b2\", [3], initializer=tf.random_normal_initializer)\n",
    "\n",
    "\n",
    "# Construct model\n",
    "z1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "y =  tf.matmul(z1, W2) + b2\n",
    "\n",
    "z1t = tf.nn.relu(tf.matmul(xt, W1) + b1)\n",
    "yt =  tf.nn.softmax(tf.matmul(z1t, W2) + b2)\n",
    "\n",
    "\n",
    "# Variables for prediction and accuracy\n",
    "prediction = tf.argmax(y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, tf.argmax(t, 1)), tf.float32))\n",
    "\n",
    "predictiont = tf.argmax(yt, 1)\n",
    "accuracyt = tf.reduce_mean(tf.cast(tf.equal(predictiont, tf.argmax(tt, 1)), tf.float32))\n",
    "\n",
    "\n",
    "# Difine the loss function\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=t, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer operation\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  loss = 126.112068\n",
      "Epoch: 10000  loss = 1.018538  diff = 0.000004292\n",
      "Epoch: 20000  loss = 0.961200  diff = 0.000004828\n",
      "Epoch: 30000  loss = 0.923840  diff = 0.000002563\n",
      "Epoch: 40000  loss = 0.900589  diff = 0.000001848\n",
      "Epoch: 49999  loss = 0.885475  diff = 0.000001490\n",
      "Optimization done\n",
      "Accuracy on train set: 0.59859157\n",
      "Accuracy on test set: 0.44444445\n"
     ]
    }
   ],
   "source": [
    "# Make an operation that initializes the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "y_value_list = []\n",
    "\n",
    "# Start a new session\n",
    "with tf.Session() as session:\n",
    "    # Initialize the values\n",
    "    session.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(50000):\n",
    "        _, loss_value = session.run([optimizer, loss], feed_dict={x: X_train, t: t_train})\n",
    "        \n",
    "        if epoch % 10000 == 0 or epoch == 49999:\n",
    "            if epoch > 1:\n",
    "                print(\"Epoch: {}  loss = {:.6f}  diff = {:.9f}\".format(epoch,loss_value,prev-loss_value))\n",
    "            else:\n",
    "                print(\"Epoch: {}  loss = {:.6f}\".format(epoch,loss_value))\n",
    "        prev = loss_value            \n",
    "            \n",
    "    print(\"Optimization done\")\n",
    "\n",
    "    # Evaluate the accuracy on the test set\n",
    "    accuracy_value = session.run(accuracy, feed_dict={x: X_train, t: t_train})\n",
    "    print(\"Accuracy on train set:\", accuracy_value)\n",
    "    # Evaluate the accuracy on the test set\n",
    "    accuracy_value = session.run(accuracyt, feed_dict={xt: X_test, tt: t_test})\n",
    "    print(\"Accuracy on test set:\", accuracy_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
